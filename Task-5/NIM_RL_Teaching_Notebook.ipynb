{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d0ea81d7",
      "metadata": {
        "id": "d0ea81d7"
      },
      "source": [
        "# ðŸ§  Reinforcement Learning with the NIM Game\n",
        "Let's teach our AI how to win a simple game using Q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2326000a",
      "metadata": {
        "id": "2326000a"
      },
      "source": [
        "## ðŸŽ® The NIM Game Rules\n",
        "- Start with 21 sticks.\n",
        "- Each player takes 1, 2, or 3 sticks on their turn.\n",
        "- The player who takes the **last stick loses**.\n",
        "\n",
        "We'll train an AI to get smarter over time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6b59538",
      "metadata": {
        "id": "d6b59538"
      },
      "outputs": [],
      "source": [
        "MAX_STICKS = 21\n",
        "ACTIONS = [1, 2, 3, 4]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "598d7dad",
      "metadata": {
        "id": "598d7dad"
      },
      "source": [
        "## ðŸ§  Step 1: Create a Q-table\n",
        "Weâ€™ll use a dictionary to store the AIâ€™s knowledge â€” the expected value (Q) of taking each action in every possible state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab88528",
      "metadata": {
        "id": "0ab88528"
      },
      "outputs": [],
      "source": [
        "Q = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c985115",
      "metadata": {
        "id": "3c985115"
      },
      "source": [
        "## ðŸŽ² Step 2: Action Choice\n",
        "Letâ€™s write a function that chooses an action. Weâ€™ll use **epsilon-greedy** â€” random at first, smarter later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e717bdea",
      "metadata": {
        "id": "e717bdea"
      },
      "outputs": [],
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def choose_action(state, epsilon):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0 for a in ACTIONS}\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice([a for a in ACTIONS if a <= state])\n",
        "    return max(Q[state], key=Q[state].get)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d01dfae",
      "metadata": {
        "id": "9d01dfae"
      },
      "source": [
        "## ðŸ’¡ Step 3: Q-Value Update Rule\n",
        "Weâ€™ll update the Q-values using this formula:\n",
        "```\n",
        "Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s') - Q(s,a))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3ed83b9",
      "metadata": {
        "id": "d3ed83b9"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def choose_action(state, epsilon):\n",
        "    if state not in Q:\n",
        "        Q[state] = {a: 0 for a in ACTIONS}\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice([a for a in ACTIONS if a <= state])\n",
        "    return max(Q[state], key=Q[state].get)\n",
        "\n",
        "# Define the update_q function here\n",
        "def update_q(state, action, reward, next_state, alpha, gamma):\n",
        "    # Ensure the next state exists in the Q table\n",
        "    if next_state not in Q:\n",
        "        # Assuming 0 is a terminal state with no possible actions\n",
        "        Q[next_state] = {a: 0 for a in ACTIONS if a <= next_state}\n",
        "        # For a terminal state like 0 sticks, there are no future actions, so max_q_next is 0\n",
        "        max_q_next = 0 if next_state == 0 else max(Q[next_state].values())\n",
        "    else:\n",
        "         # Get the maximum Q value for the next state\n",
        "         # Handle the case where next_state might be a terminal state (0 sticks)\n",
        "        max_q_next = 0 if next_state == 0 else max(Q[next_state].values())\n",
        "\n",
        "\n",
        "    # Calculate the temporal difference target\n",
        "    target = reward + gamma * max_q_next\n",
        "\n",
        "    # Calculate the error (TD error)\n",
        "    error = target - Q[state][action]\n",
        "\n",
        "    # Update the Q-value\n",
        "    Q[state][action] += alpha * error\n",
        "\n",
        "\n",
        "def train(episodes=10000, epsilon=0.3, alpha=0.1, gamma=0.9):\n",
        "    for _ in range(episodes):\n",
        "        state = MAX_STICKS\n",
        "        last_state, last_action = None, None\n",
        "\n",
        "        while state > 0:\n",
        "            action = choose_action(state, epsilon)\n",
        "            next_state = state - action\n",
        "\n",
        "            # Update Q-value for the previous state and action if available\n",
        "            if last_state is not None:\n",
        "                # The reward for the previous state's action is 0 as the game is ongoing\n",
        "                update_q(last_state, last_action, 0, state, alpha, gamma)\n",
        "\n",
        "            last_state = state\n",
        "            last_action = action\n",
        "\n",
        "            if next_state == 0:\n",
        "                # Bot took the last stick => bot loses (rewarded +1, although typically -1 for losing)\n",
        "                # Let's follow the original code's logic and use +1 here for the state where the bot wins\n",
        "                update_q(state, action, 1, next_state, alpha, gamma)\n",
        "                break\n",
        "\n",
        "            # Opponent's turn\n",
        "            valid_opponent_actions = [a for a in ACTIONS if a <= next_state]\n",
        "            if not valid_opponent_actions:\n",
        "                 # If no valid actions for opponent, it means the bot's last move left 0 or fewer sticks.\n",
        "                 # This condition should ideally not be reached if next_state check is correct,\n",
        "                 # but adding a safety update with reward 0.\n",
        "                 update_q(last_state, last_action, 0, next_state, alpha, gamma)\n",
        "                 break\n",
        "\n",
        "\n",
        "            # The opponent takes a random valid action\n",
        "            opponent_action = random.choice(valid_opponent_actions)\n",
        "            state = next_state - opponent_action # The state after the opponent's move\n",
        "\n",
        "\n",
        "            if state <= 0:\n",
        "                # Bot wins (opponent took last stick).\n",
        "                # The reward is -1 as per the comment, punishing the bot for the move\n",
        "                # that led to this winning state for the bot.\n",
        "                # We update the Q value for the bot's *last* move that led to the state *before* the opponent's winning move.\n",
        "                update_q(last_state, last_action, -1, state, alpha, gamma)\n",
        "                break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5302fc2c",
      "metadata": {
        "id": "5302fc2c"
      },
      "source": [
        "## ðŸ” Step 4: Training Loop\n",
        "Now weâ€™ll play lots of games where the AI learns from experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe1d694",
      "metadata": {
        "id": "2fe1d694"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "63bf4e99",
      "metadata": {
        "id": "63bf4e99"
      },
      "source": [
        "## ðŸš€ Train the AI!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4b8ebe",
      "metadata": {
        "id": "0c4b8ebe"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGmkPcQFjQt5",
        "outputId": "29902677-95c8-442f-aaec-5b5920f873ca"
      },
      "id": "nGmkPcQFjQt5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{21: {1: 0.6617523364268455, 2: 0.6847312049552602, 3: 0.6933591470971776, 4: 0.7020998916935944}, 19: {1: 0.4956067897042622, 2: 0.714770838222653, 3: 0.6068703388666056, 4: 0.5551990716318721}, 16: {1: 0.7289984769464327, 2: 0.721712176461752, 3: 0.7387649994031291, 4: 0.7476556632093796}, 13: {1: 0.7769580667002252, 2: 0.7859687452305363, 3: 0.8099999999999987, 4: 0.709398057894402}, 9: {1: 0.7041066299154006, 2: 0.8062768887103409, 3: 0.7469470403075547, 4: 0.899999999999999}, 5: {1: 0.34049603415999274, 2: -0.15910211283148273, 3: -0.35877876390093866, 4: -0.9988209815422262}, 3: {1: -0.38905635979667463, 2: -0.9999999999895721, 3: 0.9999999999999996, 4: 0}, 0: {}, 18: {1: 0.7155751452264201, 2: 0.6788476781050494, 3: 0.6875553077042885, 4: 0.6616735552235886}, 15: {1: 0.723050127690517, 2: 0.746061161370678, 3: 0.7515792474493079, 4: 0.7793287323551874}, 11: {1: 0.8099999999999987, 2: 0.7587399076234712, 3: 0.6964877069515032, 4: 0.7774550640594291}, 6: {1: 0.899999999999999, 2: 0.2713755712161343, 3: 0.2896542039346305, 4: 0.19802448947248197}, 14: {1: 0.7315911423133309, 2: 0.7738545713694334, 3: 0.8006083312920574, 4: 0.8099999999999987}, 2: {1: -0.9999999999999996, 2: 0.9999999999999996, 3: 0, 4: 0}, 7: {1: 0.8052383831324532, 2: 0.899999999999999, 3: 0.21882760429570752, 4: 0.152497266248587}, 17: {1: 0.7258676025792178, 2: 0.7166815047503597, 3: 0.6984377537868984, 4: 0.7501268594031298}, 12: {1: 0.7845237214863408, 2: 0.809999941901156, 3: 0.750535188256626, 4: 0.6966115657214045}, 10: {1: 0.7089145278216497, 2: 0.6955115930761334, 3: 0.6579206682153381, 4: 0.6830407637327667}, 8: {1: 0.8325540778807847, 2: 0.8059423737190403, 3: 0.899999999999999, 4: 0.4719310242846897}, 4: {1: 0.43170152399280937, 2: -0.06986835953130094, 3: -0.9999999957696215, 4: 0.9999999999999996}, 1: {1: 0.9999999999999996, 2: 0, 3: 0, 4: 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c30e744",
      "metadata": {
        "id": "2c30e744"
      },
      "source": [
        "## ðŸ§ª Letâ€™s play against the AI!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24fb8cf3",
      "metadata": {
        "id": "24fb8cf3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def play():\n",
        "    state = MAX_STICKS\n",
        "    while state > 0:\n",
        "        print(f\"Sticks left: {state}\")\n",
        "        move = int(input(\"Your move (1â€“3): \"))\n",
        "        state -= move\n",
        "        if state <= 0:\n",
        "            print(\"You took the last stick. You lose!\")\n",
        "            return\n",
        "        if state in Q:\n",
        "            ai_move = max(Q[state], key=Q[state].get)\n",
        "        else:\n",
        "            ai_move = random.choice([a for a in ACTIONS if a <= state])\n",
        "        print(f\"AI takes {ai_move} stick(s).\")\n",
        "        state -= ai_move\n",
        "        if state <= 0:\n",
        "            print(\"AI took the last stick. You win!\")\n",
        "            return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e912145f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e912145f",
        "outputId": "7f37ede7-519c-4e5c-b8bc-3486c04ac26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sticks left: 21\n",
            "Your move (1â€“3): 3\n",
            "AI takes 1 stick(s).\n",
            "Sticks left: 17\n",
            "Your move (1â€“3): 3\n",
            "AI takes 4 stick(s).\n",
            "Sticks left: 10\n",
            "Your move (1â€“3): 2\n",
            "AI takes 3 stick(s).\n",
            "Sticks left: 5\n",
            "Your move (1â€“3): 2\n",
            "AI takes 3 stick(s).\n",
            "AI took the last stick. You win!\n"
          ]
        }
      ],
      "source": [
        "play()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc857d07",
      "metadata": {
        "id": "cc857d07"
      },
      "source": [
        "## ðŸŽ‰ Summary\n",
        "You just trained an agent to play a game using trial-and-error. Thatâ€™s the magic of Reinforcement Learning!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language": "python",
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}